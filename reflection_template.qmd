---
title: "STAT 331 Portfolio"
author: "Jennifer Ibarra"
format: html
code-overflow: scroll
embed-resources: true
layout: margin-left
editor: visual
code-tools: true
code-fold: true
execute:
  eval: false
  echo: true
  warning: false
  message: false
---

[**My Grade:**]{.underline} I believe my grade equivalent to course work evidenced below to be an C.

[**Learning Objective Evidence:**]{.underline} In the code chunks below, provide code from Lab or Challenge assignments where you believe you have demonstrated proficiency with the specified learning target. Be sure to specify **where** the code came from (e.g., Lab 4 Question 2).

## Working with Data

**WD-1: I can import data from a *variety* of formats (e.g., csv, xlsx, txt, etc.).**

-   `csv` Example 1

```{r}
#| label: wd-1-csv-1

surveys <- read_csv("surveys.csv")

#lab 2 Q1

```

-   `csv` Example 2

```{r}
#| label: wd-1-csv-2

surveys <- read_csv(here("Week 2", "Lab 2", "surveys.csv"))

#the original code from lab 9 had here::here() but since we would have the here package loaded we wouldn't need that.


```

-   `xlsx`

```{r}
#| label: wd-1-xlsx

library(readxl)
military <- read_xlsx("gov_spending_per_capita.xlsx",
                      sheet = ,
                      skip  = ,
                      n_max = ,
                      na = c()
                      )

#PA4 Question 1, so why do we even hav sheet, skip, n_max, or na = c()? These options allow us to naviagate an excel sheet with multiple subsheets, header rows that aren't data, how many rows to read, and how to interpret NAs in our file.



```

**WD-2: I can select necessary columns from a dataset.**

-   Example selecting specified columns

```{r}
#| label: wd-2-ex-1
#Growing comment: Selecting columns based on their names doesn't require quotes!

#Got rid of the quotes in the 
#Lab 3 Question 5

teacher_evals_clean <- teacher_evals |>
  rename(sex = gender) |>
  filter(no_participants > 10) |>
  mutate(teacher_id = as.character(teacher_id),
         question_no = as.character(question_no)) |>
  select(course_id, 
         teacher_id, 
         question_no, 
         no_participants, 
         resp_share, 
         SET_score_avg, 
         percent_failed_cur, 
         academic_degree, 
         seniority, 
         sex) #all this

#CHECK ON MIDTERM EVAL

glimpse(teacher_evals_clean)
```

-   Example removing specified columns

```{r}
#| label: wd-2-ex-2

teacher_evals_clean <- teacher_evals |>
  rename(sex = gender) |>
  filter(no_participants > 10) |>
  mutate(teacher_id = as.character(teacher_id),
         question_no = as.character(question_no)) |>
  select(-stud_grade_avg:-stud_grade_var_coef, 
         -stud_grade_avg_cur:-maximum_score)

#alright so what if we just not selected the other columns? Ta-dah


```

-   Example selecting columns based on logical values (e.g., `starts_with()`, `ends_with()`, `contains()`, `where()`)

```{r}
#| label: wd-2-ex-3

#lab 4

childcare_long <- ca_childcare |>
  pivot_longer(cols = starts_with("mc_"), #here
    names_to = "age_group",
    names_prefix = "mc_",
    values_to = "weekly_price")


```

**WD-3: I can filter rows from a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   Numeric Example 1

```{r}
#| label: wd-3-numeric-ex-0

median_household <- ca_childcare |>
  filter(study_year %in% c(2008, 2018)) |> #right here
  group_by(census_region, study_year) |>
  summarise(median_income = median(mhi_2018, na.rm = TRUE), .groups = "drop") |>
  pivot_wider(names_from = study_year, values_from = median_income) |>
  arrange(desc("2018"))

```

-   Numeric Example 2

```{r}
#| label: wd-3-numeric-ex-1

teacher_evals_clean <- teacher_evals |>
  rename(sex = gender) |>
  filter(no_participants > 10) |> #right here
  mutate(teacher_id = as.character(teacher_id),
         question_no = as.character(question_no)) |>
  select(course_id, 
         teacher_id, 
         question_no, 
         no_participants, 
         resp_share, 
         SET_score_avg, 
         percent_failed_cur, 
         academic_degree, 
         seniority, 
         sex)

#need to find another example of this

```

-   Character Example 1 (any context)

```{r}
#| label: wd-3-character

witness1 <- person |>
  filter(str_detect(address_street_name, "Northwestern Dr")) |> 
  filter(address_number == max(address_number))

witness2 <- person |>
  filter(str_detect(address_street_name, "Franklin Ave")) |> 
  filter(str_detect(name, "Annabel"))

witnesses <- bind_rows(witness1, witness2)

```

-   Character Example 2 (example must use functions from **stringr**)

```{r}
#| label: wd-3-string

get_fit_member_suspects <- get_fit_now_member |>
  filter(str_detect(id, "^48Z"))  

get_fit_check_suspects <- get_fit_now_check_in |>
  filter(str_detect(membership_id, "^48Z")) |>  
  filter(check_in_date == 20180109)

suspects_combined <- get_fit_check_suspects |>
  inner_join(get_fit_member_suspects, by = c("membership_id" = "id"))

```

-   Date (example must use functions from **lubridate**)

```{r}
#| label: wd-3-date

sql_symphony_attendees <- facebook_event_checkin |>
 filter(str_detect(event_name, 
                   "SQL Symphony Concert"), 
                   year(date) == 2017, 
                   month(date) == 12)


```

**WD-4: I can modify existing variables and create new variables in a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   Numeric Example 1

```{r}
#| label: wd-4-numeric-ex-1

#lab 3 question 1

summary_table <- evals |>
  filter(question_no == 903) |>
  mutate(
    SET_level = if_else(SET_score_avg >= 4, "excellent", "standard"),
    sen_level = case_when(
      seniority <= 4 ~ "junior",          
      seniority <= 8 ~ "senior",          
      seniority > 8  ~ "very senior")) |>
  select(course_id, SET_level, sen_level) |>
  count(sen_level, name = "n") |>
  mutate(prop = n / sum(n),
         variable = "Seniority Level",
         level = sen_level) |>
  select(variable, level, n, prop) |>
  kable(caption = "Distribution of Academic Degree",
        col.names = c("Variable", 
                      "Level", 
                      "n", 
                      "prop"), bold = TRUE) |>
  kable_styling(
    bootstrap_options = c("striped", "condensed")) 

```

-   Numeric Example 2

```{r}
#| label: wd-4-numeric-ex-2

ep = rnorm(n = 100, mean = 0, sd = sqrt(1))

#okay hear me out! I am technically creating a numeric variable here by making by epsilon with the random normal distribution function.

```

-   Factor Example 1 (renaming levels)

```{r}
#| label: wd-4-factor-ex-1

#lab 4 question 7

childcare_long <- childcare_long |>
  mutate(
    census_region = fct_reorder(census_region, weekly_price, .fun = median, na.rm = TRUE),
    age_group = factor(age_group) |> 
      fct_recode(
        "Infant"      = "infant",
        "Toddler"     = "toddler",
        "Preschooler" = "preschool"
      )
  )



```

-   Factor Example 2 (reordering levels)

```{r}
#| label: wd-4-factor-ex-2

childcare_long <- childcare_long |>
  mutate(census_region = fct_reorder(census_region, weekly_price, .fun = median, na.rm = TRUE),
    age_group = str_to_title(age_group))

```

-   Character (example must use functions from **stringr**)

```{r}
#| label: wd-4-string

#lab 4 question 4

ca_childcare <- ca_childcare |> 
  mutate(county_name = str_remove(county_name, " County")) |> #here
  mutate(census_region = fct_collapse(county_name,
    "Superior California" = superior_counties,
    "North Coast" = north_coast_counties,
    "San Francisco Bay Area" = san_fran_counties,
    "Northern San Joaquin Valley" = n_san_joaquin_counties,
    "Central Coast" = central_coast_counties,
    "Southern San Joaquin Valley" = s_san_joaquin_counties,
    "Inland Empire" = inland_counties,
    "Los Angeles" = la_county,
    "Orange" = orange_county,
    "San Diego/Imperial" = san_diego_imperial_counties))

```

-   Date (example must use functions from **lubridate**)

```{r}
#| label: wd-4-date


#Growing Comment: It looks like you are using as.Date() to parse your dates. This is a tool that was not explicitly taught in lecture / lab. While this tool accomplishes this task, that is not the intention of this assignment. You are being assessed on your understanding of the tools that have been taught in this course. In the future, please use tools that have been taught in the course when addressing Lab questions.

# facebook_event_checkin <- facebook_event_checkin |>
#   mutate(date = as.Date(as.character(date), format = "%Y%m%d"))

#I have revised my code to use functions from the lubridate package. 

facebook_event_checkin <- facebook_event_checkin |> 
  mutate(date = ymd(as.character(date)))

```

**WD-5: I can use mutating joins to combine multiple dataframes.**

-   `left_join()` Example 1

```{r}
#| label: wd-5-left-ex-1

ca_childcare <- childcare_costs |> 
                left_join(x = childcare_costs,
                          y = counties,
                          by = join_by(county_fips_code == county_fips_code)) |>
                filter(state_name == "California")

```

-   `right_join()` Example 1

```{r}
#| label: wd-5-right

witness1 <- person |>
  filter(str_detect(address_street_name, "Northwestern Dr")) |> 
  filter(address_number == max(address_number))

witness2 <- person |>
  filter(str_detect(address_street_name, "Franklin Ave")) |> 
  filter(str_detect(name, "Annabel"))

witnesses <- bind_rows(witness1, witness)

#so this worked but we can add a right join here for fun by just

witness_join <- right_join(person, witnesses, by ="person_id")

#now it is in one dataframe

```

-   `left_join()` **or** `right_join()` Example 2

```{r}
#| label: wd-5-left-right-ex-2

ca_childcare <- ca_childcare |>
              left_join ( x = ca_childcare,
                          y = tax_rev_clean,
                          by = join_by(county_name == entity_name)) |>
              group_by(county_name)

```

-   `inner_join()` Example 1

```{r}
#| label: wd-5-inner-ex-1

witnesses <- witnesses |>
  inner_join(interview, by = c("id" = "person_id"))
```

-   `inner_join()` Example 2

```{r}
#| label: wd-5-inner-ex-2

suspects_combined <- get_fit_check_suspects |>
  inner_join(get_fit_member_suspects, by = c("membership_id" = "id"))

```

**WD-6: I can use filtering joins to filter rows from a dataframe.**

-   `semi_join()`

```{r}
#| label: wd-6-semi

innerwitnesses <- witnesses |>
  inner_join(interview, by = c("id" = "person_id"))
innerwitnesses

semiwitnesses <- witnesses |>
  semi_join(interview, by = c("id" = "person_id"))
semiwitnesses

#using a semijoin will get us the same result


```

-   `anti_join()`

```{r}
#| label: wd-6-anti

#my before code from Lab 5 filtering attendees that attended the symphony more than or exactly 3 times
frequent_sql_attendees <- sql_symphony_attendees |>
  group_by(person_id) |>
  summarise(times_attended = n()) |>
  filter(times_attended >= 3)

frequent_sql_attendees

#then I checked if that attendee had an interview and if they had a gym membership
#person_id is 24556 and 99716

interview_check <- interview |>
  filter(person_id == "99716 | 24556")

interview_check

#rats so no interview which means this person 
#is in the wild...let's check the fit now membership 
#and see if this person has a membership?

get_fit_now_member_check <- get_fit_now_member |>
  filter(person_id == "24556")

get_fit_now_member_check

#using an antijoin here would have told me who attended the concert and does NOT have a gym membership and doesn't have an interview. Since the murderer is still at large they wouldn't have a interview and the gym membership was just a hunch from the personal ID. 

fit_members <- get_fit_now_member |> select(person_id)

concert_no_fit <- frequent_sql_attendees |>
  anti_join(fit_members, by = "person_id")

# #https://chatgpt.com/
# In the **SQL Murder Mystery (‚ÄúSQL City‚Äù)** challenge, an **anti-join** is most useful at the point where you need to find **which suspects *do not* appear somewhere they should** ‚Äî in other words, when eliminating people based on missing evidence.
# 
# ### ‚≠ê Best Place to Use an Anti-Join
# 
# The clearest spot is when you‚Äôre trying to answer questions like:
# 
# > **‚ÄúWhich people in the `person` table *don‚Äôt* appear in the list of valid suspects derived from clues?‚Äù**
# 
# More specifically, many solvers use an **anti-join** when narrowing down suspects by excluding anyone who **doesn‚Äôt have matching evidence in related tables**, such as:
# 
# * People **not** found in the `income` table when looking for someone wealthy
# * People **not** holding a gym membership
# * People who **didn‚Äôt appear** at the crime scene based on `interview` or `facebook_event_checkin` tables
# * People **not** matching physical clues like height, hair color, etc.
# 
# ### üí° Concrete Example
# 
# Suppose you‚Äôve determined from the clues that suspects must have a gym membership. An anti-join can give you the people who *don‚Äôt*:
# 
# ```sql
# SELECT p.*
# FROM person p
# LEFT JOIN get_fit_now_member g
#     ON p.id = g.person_id
# WHERE g.person_id IS NULL;
# ```
# 
# This returns **people who do not have a Get Fit Now membership** ‚Äî effectively removing people who *can't* be the suspect.
# 
# ### üß† Why Anti-Joins Help
# 
# Anti-joins let you turn logic like:
# 
# * ‚ÄúWe know the killer **must** have X‚Äù
# 
# into:
# 
# * ‚ÄúRemove everyone who **doesn‚Äôt** have X.‚Äù
# 
# This dramatically reduces the search space and makes it easier to confirm the final culprit.
# 
# ---
# 
# If you want, I can walk you through exactly where to apply it step-by-step in the SQL Murder Mystery.


```

**WD-7: I can pivot dataframes from long to wide and visa versa**

-   `pivot_longer()`

```{r}
#| label: wd-7-long

childcare_long <- ca_childcare |>
  pivot_longer(cols = starts_with("mc_"),  
    names_to = "age_group",
    names_prefix = "mc_",
    values_to = "weekly_price")

```

-   `pivot_wider()`

```{r}
#| label: wd-7-wide

median_household <- ca_childcare |>
  filter(study_year %in% c(2008, 2018)) |>
  group_by(census_region, study_year) |>
  summarise(median_income = median(mhi_2018, na.rm = TRUE), .groups = "drop") |>
  pivot_wider(names_from = study_year, values_from = median_income) |>
  arrange(desc("2018"))


```

## Reproducibility

**R-1: I can create professional looking, reproducible analyses using RStudio projects, Quarto documents, and the here package.**

The following assignments satisfy the above criteria:

-   Lab 7
-   Lab 7 Challenge
-   Lab 8
-   Lab 9
-   Lab 10

**R-2: I can write well documented and tidy code.**

-   Example of **ggplot2** plotting

```{r}
#| label: r-2-1

ggplot(data = surveys, 
       mapping = aes(x = weight, y = hindfoot_length)) +
  geom_point(color = "salmon", alpha = 0.5) +
  facet_wrap(~species) +
  labs( x= "Weight (g)",y = NULL, title = "Relationship between weight and hindfoot length across rodent species", subtitle = "Hindfoot Length (mm)") +
  theme(plot.subtitle = element_text(hjust = 0.5))

```

-   Example of **dplyr** pipeline

```{r}
#| label: r-2-2

median_household <- ca_childcare |>
  filter(study_year %in% c(2008, 2018)) |>
  group_by(census_region, study_year) |>
  summarise(median_income = median(mhi_2018, na.rm = TRUE), .groups = "drop") |>
  pivot_wider(names_from = study_year, values_from = median_income) |>
  arrange(desc("2018"))

```

-   Example of function formatting

```{r}
#| label: r-2-3

#Lab 10 Q #8

mycifun <- function(beta0 = 2, beta1 = 1, n = 100) {
  

# generate x vector

x = runif(n, min = 1, max = 100)


# generate noise `ep` vector

ep = rnorm(n = n, mean = 0, sd = sqrt(1))


# generate outcome from population model
y = beta0 + x*beta1 + ep


# create an "observed data" dataframe with only the x and y vectors

observed_data <- tibble(x,y)


observed_data_lm <- lm(y ~ x,
                       data = observed_data) 

extract_observed <- tidy(observed_data_lm,
                         conf.int = TRUE,
                         conf.level = 0.95) |>
                    filter(term == "x") |>
                    select(estimate, conf.low, conf.high) 

extract_observed |>
 mutate(cover = if_else(conf.high > beta1, TRUE, FALSE)) }



```

**R-3: I can write robust programs that are resistant to changes in inputs.**

-   Example (any context)

```{r}
#| label: r-3-example

randomBabies <- function(n_babies){
  
      babies <- rbinom(1, n_babies, 1/n_babies)

}

#important to start with 1 simulation so that we can switch it up
```

-   Example (function stops)

```{r}
#| label: r-3-function-stops

rescale_01 <- function(x) {
    if(!is.numeric(x)) {stop("input vector is not numeric")}

    if(length(x) <=1) {stop("the length of the input vector is not greater than one")}

    # (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
  
      r <- range(x, na.rm = TRUE)
        (x - r[1]) / (r[2] - r[1])

}


```

## Data Visualization & Summarization

**DVS-1: I can create visualizations for a *variety* of variable types (e.g., numeric, character, factor, date)**

-   At least two numeric variables

```{r}
#| label: dvs-1-num

ggplot(ca_childcare, aes(x = mhi_2018, y = mc_infant)) +
  geom_point(alpha = 0.6, color = "pink") +
  geom_smooth(method = "lm", se = TRUE, color = "darkgreen") +
  labs(
    title = "Relationship Between Median Household Income and Infant Center-Based Childcare Prices",
    x = "Median Household Income in 2018 ($)",
    y = "Weekly Median Infant Care Price ($)")


```

-   At least one numeric variable and one categorical variable

```{r}
#| label: dvs-2-num-cat

ggplot(data = surveys, 
       mapping = aes(y = species, x = weight)) +
  geom_jitter(color = "grey", alpha = 0.5) +
  geom_boxplot(aes(fill = sex), outliers = FALSE) +
  scale_fill_brewer(palette = 5)
  labs( y= "Species of Rodent",x = "Weight (g)", title = "Relationship between species of rodents and their weight")

```

-   At least two categorical variables

```{r}
#| label: dvs-2-cat

sen_vs_jun |> ggplot(aes(x = sen_level, fill = SET_level)) +
  geom_bar() +
  scale_fill_manual( values = c("standard" = "#C4a484", "excellent" =    "#CBC3E3"))+
  labs(title = "Evaluations of Teacher's Use of Activities", x = "Years of Experience", y = NULL)

```

-   Dates (time series plot)

```{r}
#| label: dvs-2-date

ggplot(summary_fishNA, aes(x = year, y = NAs_inweight, color = section, shape = section)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_smooth(method = "loess", se = FALSE, linewidth = 1) +
  facet_wrap(~ trip) +
  scale_color_manual(values = c("blue", "orange")) +
   labs(title = "Amount of NAs over time by section leader",
    x = "Years",
    y = NULL,
    color = "Section Leader") +
  theme_minimal(base_size = 8) +
  theme(legend.position = "right",aspect.ratio = 1,strip.text = element_text(face = "bold")) +
  theme_gray()
```

**DVS-2: I use plot modifications to make my visualization clear to the reader.**

-   I can modify my plot theme to be more readable

```{r}
#| label: dvs-2-ex-1

```

-   I can modify my colors to be accessible to anyone's eyes

```{r}
#| label: dvs-2-ex-2

```

-   I can modify my plot titles to clearly communicate the data context

```{r}
#| label: dvs-2-ex-3

```

-   I can modify the text in my plot to be more readable

```{r}
#| label: dvs-2-ex-4

#before code chunk, I did not change the font size and I did not separate
ggplot(childcare_long, aes(x = study_year, y = weekly_price, color = census_region)) +
  geom_point(alpha = 0.4, size = 1) +
  geom_smooth(method = "loess", se = FALSE, linewidth = 1) +
  facet_wrap(~ age_group) +
  labs(title = "Weekly Median Price for Center-Based Childcare ($)",
    x = "Study Year",
    y = NULL,
    color = "California Region") +
  theme_minimal(base_size = 9) +
  theme(legend.position = "right",aspect.ratio = 1,strip.text = element_text(face = "bold")) 

#here is what my code should have included along with changing the font size to 9 instead of 12

+ scale_x_continuous(breaks = seq(2008, 2018, by = 2), expand = expansion(mult = c(0.05,0.05))) +
  scale_y_continuous(labels = dollar)
```

-   Before and after
-   
-   ![](images/clipboard-1844204121.png)![](images/clipboard-739741966.png)
-   I can reorder my legend to align with the colors in my plot

```{r}
#| label: dvs-2-ex-5

```

**DVS-3: I show creativity in my visualizations**

-   I can use non-standard colors (Example 1)

```{r}
#| label: dvs-3-1-ex-1

```

-   I can use non-standard colors (Example 2)

```{r}
#| label: dvs-3-1-ex-2

ggplot(mean_fish, aes(x = year, 
                       y = mean_fk, 
                       color = species, 
                       shape = species)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_smooth(method = "loess", se = FALSE, linewidth = 1) +
  scale_color_manual(values = c("WCT" = "coral", 
                                "RBT" = "salmon", 
                                "Bull" = "darkseagreen", 
                                "Brown" = "saddlebrown"))
   labs(title = "Change in Montana Fish Health Over Time (Using Fulton's K)",
    x = "Years",
    y = NULL,
    color = "species") +
  theme_minimal(base_size = 8) +
  theme(legend.position = "right",aspect.ratio = 1,strip.text = element_text(face = "bold")) +
  theme_gray()

```

-   I can use annotations (e.g., `geom_text()`)

```{r}
#| label: dvs-3-2

```

-   I can choose creative geometries (e.g., `geom_segment()`, `geom_ribbon)()`)

```{r}
#| label: dvs-3-3

ci_dat <- ci_dat |>
  mutate(MultiverseNum = 1:100, 
         inside_bounds = conf.low <=1 & conf.high > 1) |> 
  ggplot(aes(x = conf.low, 
             xend = conf.high, 
             y = MultiverseNum, 
             yend = MultiverseNum, 
             color = inside_bounds )) +
  geom_segment() +
   scale_color_manual(values = c(`TRUE` = "darkgreen", `FALSE` = "magenta"))+
 geom_point(aes(x = estimate, y = MultiverseNum), color = "black", size = 1) +
  labs(x = "Slope Coefficient Estimate", y = NULL,
title = "95% Confidence Intervals of Estimated Regression Slope for 100 Multiverses", 
subtitle =
"CI's that do not include the population slope (1) are magenta")

```

**DVS-4: I can calculate numerical summaries of variables.**

-   Example using `summarize()`

```{r}
#| label: dvs-4-summarize

ci_dat |>
summarize(porp = sum(cover)/length(cover))
```

-   Example using `across()`

```{r}
#| label: dvs-4-across

fish |>
  select(everything()) |>  
  summarize(across(everything(), ~ sum(is.na(.))))

```

**DVS-5: I can find summaries of variables across multiple groups.**

-   Example 1

```{r}
#| label: dvs-5-1

frequent_sql_attendees <- sql_symphony_attendees |>
  group_by(person_id) |>
  summarise(times_attended = n()) |>
  filter(times_attended >= 3)

```

-   Example 2

```{r}
#| label: dvs-5-2

teacher_evals_clean |>
  filter(academic_degree %in% c("dr", "prof")) |>
  group_by(teacher_id, seniority, sex) |>
  summarise(avg_response_rate = mean(resp_share, na.rm = TRUE), 
            .groups = "drop") |>
  slice_min(avg_response_rate, n = 1) |>
  bind_rows(teacher_evals_clean |>
      filter(academic_degree %in% c("dr", "prof")) |>
      group_by(teacher_id, seniority, sex) |>
      summarise(avg_response_rate = mean(resp_share, na.rm = TRUE), 
                .groups = "drop") |>
      slice_max(avg_response_rate, n = 1))

```

**DVS-6: I can create tables which make my summaries clear to the reader.**

-   I can modify my column names to clearly communicate the data context

```{r}
#| label: dvs-6-ex-1

fish_NA_sum <-map_int(fish, ~sum(is.na(.x))) |>
              enframe(name = "Variable", value = "NA Counts")

```

-   I can modify the text in my table to be more readable (e.g., bold face for column headers)

```{r}
#| label: dvs-6-ex-2

surveys |>
        map_chr(class) |>
        enframe(name = "Variable", value = "Data Type") |>
        arrange("Variable") |>
        kable(caption = "Summary of Data Types in Surveys Dataset") |>
        kable_styling() |>
        row_spec(0, bold = TRUE)

#the kable() function already bolds automatically but kable_styling helps narrow down which row we want bolded

```

-   I can arrange my table to have an intuitive ordering

```{r}
#| label: dvs-6-ex-3

surveys |>
        map_chr(class) |>
        enframe(name = "Variable", value = "Data Type") |>
        arrange("Variable") |> #here
        kable(caption = "Summary of Data Types in Surveys Dataset")

```

**DVS-7: I show creativity in my tables.**

-   I can use non-default colors

```{r}
#| label: dvs-7-ex-1

surveys |>
        map_chr(class) |>
        enframe(name = "Variable", value = "Data Type") |>
        arrange("Variable") |>
        kable(caption = "Summary of Data Types in Surveys Dataset") |>
        kable_styling() |>
        row_spec(0, bold = TRUE, background = "lightpink")
#I just made the header pink but if I alter that 0 to a different row number then that row would be pink.
#https://r-graph-gallery.com/362-customize-colors-in-kableExtra.html

```

-   I can modify the layout of my table to be more readable (e.g., `pivot_longer()` or `pivot_wider()`)

```{r}
#| label: dvs-7-ex-2

pivot_table <- function(data, rows, cols) {
  data |> 
    count(pick(c({{ rows }}, {{ cols }}))) |> 
    pivot_wider(
      names_from = {{ cols }}, 
      values_from = n, #n is coming from the result of count()
      names_sort = TRUE,
      values_fill = 0
    )
}

```

## Program Efficiency

**PE-1: I can write concise code which does not repeat itself.**

-   using a single function call with multiple inputs (rather than multiple function calls)

```{r}
#| label: pe-1-one-call

# witness1 <- person |>
#   filter(str_detect(address_street_name, regex("Northwestern Dr", ignore_case = TRUE))) |> 
#   filter(address_number == max(address_number))

# witness2 <- person |>
#   filter(str_detect(address_street_name, regex("Franklin Ave", ignore_case = TRUE))) |> 
#   filter(str_detect(name, "Annabel"))

# witnesses <- bind_rows(witness1, witness2)

#the code above could be fixed by maximizing the use of the filter function. We could instead combine the filters per witness instead of a new filter line for each row. 

witness1 <- person |>
  filter(
    str_detect(address_street_name, "Northwestern Dr"),
    address_number == max(address_number)
  )

witness2 <- person |>
  filter(
    str_detect(address_street_name, "Franklin Ave"),
    str_detect(name, "Annabel")
  )

witnesses <- bind_rows(witness1, witness2)


```

-   using `across()`

```{r}
#| label: pe-1-across

tax_rev_clean <- tax_rev |>
                group_by(entity_name) |>
                summarise(across(everything(), first), .groups = "drop")

```

-   using functions from the `map()` family

```{r}
#| label: pe-1-map-1

#Lab 9 Q4
#Success comment: 
# Do you need to select for this to work? Can map_at() work on the entire dataset?

teacher_evals_clean <- evals |>
 # rename(sex = gender) |> 
  filter(no_participants > 10) |>
  mutate(teacher_id = as.character(teacher_id),
         question_no = as.character(question_no)) |>
  select("course_id", "teacher_id", "question_no", "no_participants", "resp_share", "SET_score_avg", "percent_failed_cur", "academic_degree", "seniority", "sex") |>
  map_at(c("course_id", #new code starts here, chef
           "weekday", 
           "academic_degree", 
           "time_of_day", 
            "sex"),
          as.factor) |>
  bind_cols()

#not at this point! Just because we are just being asked to convert some columns to factors.

teacher_evals_clean <- evals |>
 # rename(sex = gender) |> 
  filter(no_participants > 10) |>
  mutate(teacher_id = as.character(teacher_id),
         question_no = as.character(question_no)) |>
  map_at(c("course_id", #new code starts here, chef
           "weekday", 
           "academic_degree", 
           "time_of_day", 
            "sex"),
          as.factor) |>
  bind_cols()

#we can later select or remove columns we do or don't need for later analyses
```

**PE-2: I can write functions to reduce repetition in my code.**

-   Example 1: Function that operates on vectors

```{r}
#| label: pe-2-1

imposs_NA <- function(x, min, max) {
  case_when(
    x < min ~ NA,
    x > max ~ NA,
    .default = x
  )
}

imposs_NA(1:10, min = 3, max = 7)

```

-   Example 2: Function that operates on data frames

```{r}
#| label: pe-2-2

rescale_column <- function(df, cols){
  df |> 
    mutate(
      across(.cols = {{ cols }}, 
             .fns = ~ rescale_01(.x)
             )
      )
}

```

-   Example 3: Function that operates on vectors *or* data frames

```{r}
#| label: pe-2-3

plot_function <- function(df, x, y, color) {
  
  label <- rlang::englue("Flipper length vs Body mass across penguin species")
  
  df |>
    ggplot(aes(x = {{ x }}, y = {{ y }}, color = {{ color }})) +
    geom_point() +
    labs(title = label, 
         x = " Flipper length (mm)", 
         y = " Body mass (g) ")
  
}
```

**PE-3:I can use iteration to reduce repetition in my code.**

-   using `across()`

```{r}
#| label: pe-3-across


rescale_column <- function(df, cols){
  df |> 
    mutate(
      across(.cols = {{ cols }}, 
             .fns = ~ rescale_01(.x)
             )
      )
}
```

-   using a `map()` function with **one** input (e.g., `map()`, `map_chr()`, `map_dbl()`, etc.)

```{r}
#| label: pe-3-map-1

surveys |>
        map_chr(class) |>
        enframe(name = "Variable", value = "Data Type") |>
        arrange("Variable") |>
        kable(caption = "Summary of Data Types in Surveys Dataset",
              bold = TRUE)

```

-   using a `map()` function with **one** input

```{r}
#| label: pe-3-map-2

ci_dat <- map(.x = 1:1000,
              .f = ~mycifun(beta0 = 3, beta1 = 0.5, n = 100)
              )

```

**PE-4: I can use modern tools when carrying out my analysis.**

-   I can use functions which are not superseded or deprecated

```{r}
#| label: pe-4-1

```

-   I can connect a data wrangling pipeline into a `ggplot()`

```{r}
#| label: pe-4-2

ci_dat <- ci_dat |>
  map(.x = 1:100,
      .f = ~mycifun(beta0 = 0, beta1 = 1, n = 100)) |>
  bind_rows() |>
  mutate(MultiverseNum = 1:100, 
         inside_bounds = conf.low <=1 & conf.high > 1) |> 
  ggplot(aes(x = conf.low, 
             xend = conf.high, 
             y = MultiverseNum, 
             yend = MultiverseNum, 
             color = inside_bounds )) +
  geom_segment() +
   scale_color_manual(values = c(`TRUE` = "darkgreen", `FALSE` = "magenta"))+
 geom_point(aes(x = estimate, y = MultiverseNum), color = "black", size = 1) +
  labs(x = "Slope Coefficient Estimate", y = NULL,
title = "95% Confidence Intervals of Estimated Regression Slope for 100 Multiverses", 
subtitle =
"CI's that do not include the population slope (1) are magenta")

```

## Data Simulation & Statistical Models

**DSSM-1: I can simulate data from a *variety* of probability models.**

-   Example 1

```{r}
#| label: dsm-1-1

randomBabies <- function(n_babies){
  
      babies <- rbinom(1, n_babies, 1/n_babies)

}
  
set.seed(918)

results <- map_int(.x = 1:10000,
                   .f = ~ randomBabies(n_babies = 4)
                  )

table(results)

```

-   Example 2

```{r}
#| label: dsm-1-2

mycifun <- function(beta0 = 2, beta1 = 1, n = 100) {
  

# generate x vector

x = runif(n, min = 1, max = 100)


# generate noise `ep` vector

ep = rnorm(n = n, mean = 0, sd = sqrt(1))


# generate outcome from population model
y = beta0 + x*beta1 + ep


# create an "observed data" dataframe with only the x and y vectors

observed_data <- tibble(x,y)


observed_data_lm <- lm(y ~ x,
                       data = observed_data) 

extract_observed <- tidy(observed_data_lm,
                         conf.int = TRUE,
                         conf.level = 0.95) |>
                    filter(term == "x") |>
                    select(estimate, conf.low, conf.high) 

extract_observed |>
 mutate(cover = if_else(conf.high > beta1, TRUE, FALSE)) }

```

**DSSM-2: I can conduct common statistical analyses in R.**

-   Example 1

```{r}
#| label: dsm-2-1

species_mod <- aov(weight ~ species, data = surveys)

#response ~ predictor

summary(species_mod)

#lab 2 question 17

```

-   Example 2

```{r}
#| label: dsm-2-2

#chi_data <- table(sen_vs_jun$sen_level, sen_vs_jun$SET_level)

# view(chi_data)

# chisq.test(chi_data)

#Growing Comment: This works, but did you need to make a contingency table? Could you have input these vectors directly into the chisq.test() function?

chisq.test(sen_vs_jun$sen_level, sen_vs_jun$SET_level)

#Pearson's Chi-squared test

# data:  sen_vs_jun$sen_level and sen_vs_jun$SET_level
# X-squared = 10.207, df = 2, p-value = 0.006075

```

-   Example 3

```{r}
#| label: dsm-2-3

t.test(ToothGrowth$len)

#Growing comment: Comments
#This is not a t-test for the difference in means between two groups. This test needs to have two variables (1) the length of the teeth, and (2) the group that the guinea pig was in.

#You also need to make sure you are using unequal variances.

#so I need to alter my code to include a ~ variable or group the guinea pigs were in and tell it that I have unequal variances (var.equal = FALSE). 

t.test(len ~ supp, data = ToothGrowth, var.equal = FALSE)

# 	Welch Two Sample t-test
# 
# data:  len by supp
# t = 1.9153, df = 55.309, p-value = 0.06063
# alternative hypothesis: true difference in means between group OJ and group VC is not equal to 0
# 95 percent confidence interval:
#  -0.1710156  7.5710156
# sample estimates:
# mean in group OJ mean in group VC 
#         20.66333         16.96333 

#Lab 1 Q #10

```

## Revising My Thinking

<!-- How did you revise your thinking throughout the course? How did you revise your thinking on the code examples you have provided in your portfolio? -->

I think one of the big themes I ran with for coding was how can I be concise and straightforward with my code. There are also plenty of resources like stackoverflow that can get me unstuck rather than using AI. It does feel more satisfactory and relieving to have figure it out myself.

<!-- For the revisions included in your Portfolio, to help me understand the nature of your revisions, please denote somehow the feedback I provided you (e.g., boldface, italics, colored text) before your revisions. -->

## Extending My Thinking

<!-- How did you extended your thinking throughout the course? How did you extend your thinking on the code examples you have provided in your portfolio? -->

I think broadening my resources was really valuable to me. I use to really look at ChatGPT for support but it did feel dirty! Chat I think relies a lot on base code which is scary because at some point those functions will become superseded or deprecated. I also think that this challenged me to be more vocal about needing support. Coming to grad school from the start has felt competitive so it's nice to finally break through that fallacy and advocate for myself when I need the support.

## Peer Support & Collaboration

<!-- Include an image or a description of feedback you gave that you are proud of (either in a peer review or in Discord). -->

I think that to be able to master a subject you need to be able to teach someone exactly what you know. So in this case where I didn't understand a whole lot I wanted to make sure I can ask my partner to explain why they want to use a function but not sound like I am challenging them. I think vulnerability is a strong point and the squeaky wheel gets the grease!

<!-- Include a description of how you grew as a collaborator through the weekly pair programming activities.   -->

I think I grew as a collaborator by coming to class more prepared with the check ins. I didn't find the book to be helpful until I started reading it, duh. But keeping in mind that the following Tuesday I am going to have to potentially support someone in this class really kept me on my toes. I think I got tired of taking information from others and not being able to give back.
