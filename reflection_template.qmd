---
title: "STAT 331 Portfolio"
author: "Jennifer Ibarra"
format: html
code-overflow: scroll
embed-resources: true
layout: margin-left
editor: visual
execute: 
  eval: false 
  echo: true
---

[**My Grade:**]{.underline} I believe my grade equivalent to course work evidenced below to be an D.

[**Learning Objective Evidence:**]{.underline} In the code chunks below, provide code from Lab or Challenge assignments where you believe you have demonstrated proficiency with the specified learning target. Be sure to specify **where** the code came from (e.g., Lab 4 Question 2).

## Working with Data

**WD-1: I can import data from a *variety* of formats (e.g., csv, xlsx, txt, etc.).**

-   `csv` Example 1

```{r}
#| label: wd-1-csv-1

surveys <- read_csv("surveys.csv")

#lab 2 Q1

```

-   `csv` Example 2

```{r}
#| label: wd-1-csv-2

surveys <- read_csv(here("Week 2", "Lab 2", "surveys.csv"))

#the original code from lab 9 had here::here() but since we would have the here package loaded we wouldn't need that.


```

-   `xlsx`

```{r}
#| label: wd-1-xlsx

library(readxl)
military <- read_xlsx("gov_spending_per_capita.xlsx",
                      sheet = ,
                      skip  = ,
                      n_max = ,
                      na = c()
                      )

#PA4 Question 1, so why do we even hav sheet, skip, n_max, or na = c()? These options allow us to naviagate an excel sheet with multiple subsheets, header rows that aren't data, how many rows to read, and how to interpret NAs in our file.



```

**WD-2: I can select necessary columns from a dataset.**

-   Example selecting specified columns

```{r}
#| label: wd-2-ex-1
#Growing comment: Selecting columns based on their names doesn't require quotes!

#Got rid of the quotes in the 
#Lab 3 Question 5

teacher_evals_clean <- teacher_evals |>
  rename(sex = gender) |>
  filter(no_participants > 10) |>
  mutate(teacher_id = as.character(teacher_id),
         question_no = as.character(question_no)) |>
  select(course_id, 
         teacher_id, 
         question_no, 
         no_participants, 
         resp_share, 
         SET_score_avg, 
         percent_failed_cur, 
         academic_degree, 
         seniority, 
         sex) #all this

#CHECK ON MIDTERM EVAL

glimpse(teacher_evals_clean)
```

-   Example removing specified columns

```{r}
#| label: wd-2-ex-2

teacher_evals_clean <- teacher_evals |>
  rename(sex = gender) |>
  filter(no_participants > 10) |>
  mutate(teacher_id = as.character(teacher_id),
         question_no = as.character(question_no)) |>
  select(-stud_grade_avg:-stud_grade_var_coef, 
         -stud_grade_avg_cur:-maximum_score)

#alright so what if we just not selected the other columns? Ta-dah


```

-   Example selecting columns based on logical values (e.g., `starts_with()`, `ends_with()`, `contains()`, `where()`)

```{r}
#| label: wd-2-ex-3

#lab 4

childcare_long <- ca_childcare |>
  pivot_longer(cols = starts_with("mc_"), #here
    names_to = "age_group",
    names_prefix = "mc_",
    values_to = "weekly_price")

#CHECK ON MIDTERM EVAL

```

**WD-3: I can filter rows from a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   Numeric Example 1

```{r}
#| label: wd-3-numeric-ex-0

median_household <- ca_childcare |>
  filter(study_year %in% c(2008, 2018)) |> #right here
  group_by(census_region, study_year) |>
  summarise(median_income = median(mhi_2018, na.rm = TRUE), .groups = "drop") |>
  pivot_wider(names_from = study_year, values_from = median_income) |>
  arrange(desc("2018"))

```

-   Numeric Example 2

```{r}
#| label: wd-3-numeric-ex-1

teacher_evals_clean <- teacher_evals |>
  rename(sex = gender) |>
  filter(no_participants > 10) |> #right here
  mutate(teacher_id = as.character(teacher_id),
         question_no = as.character(question_no)) |>
  select(course_id, 
         teacher_id, 
         question_no, 
         no_participants, 
         resp_share, 
         SET_score_avg, 
         percent_failed_cur, 
         academic_degree, 
         seniority, 
         sex)

#need to find another example of this

```

-   Character Example 1 (any context)

```{r}
#| label: wd-3-character

witness1 <- person |>
  filter(str_detect(address_street_name, regex("Northwestern Dr", ignore_case = TRUE))) |> 
  filter(address_number == max(address_number))

witness2 <- person |>
  filter(str_detect(address_street_name, regex("Franklin Ave", ignore_case = TRUE))) |> 
  filter(str_detect(name, "Annabel"))

witnesses <- bind_rows(witness1, witness2)

```

-   Character Example 2 (example must use functions from **stringr**)

```{r}
#| label: wd-3-string

get_fit_member_suspects <- get_fit_now_member |>
  filter(str_detect(id, regex("^48Z", ignore_case = TRUE)))


get_fit_check_suspects <- get_fit_now_check_in |>
  filter(str_detect(membership_id, regex("^48Z", ignore_case = TRUE))) |>
  filter(check_in_date == 20180109)

suspects_combined <- get_fit_check_suspects |>
  inner_join(get_fit_member_suspects, by = c("membership_id" = "id"))

```

-   Date (example must use functions from **lubridate**)

```{r}
#| label: wd-3-date

sql_symphony_attendees <- facebook_event_checkin |>
 filter(str_detect(event_name, regex("SQL Symphony Concert", ignore_case = TRUE)),
   lubridate::year(date) == 2017,
   lubridate::month(date) == 12)

```

**WD-4: I can modify existing variables and create new variables in a dataframe for a *variety* of data types (e.g., numeric, integer, character, factor, date).**

-   Numeric Example 1

```{r}
#| label: wd-4-numeric-ex-1

#lab 3 question 1

summary_table <- evals |>
  filter(question_no == 903) |>
  mutate(
    SET_level = if_else(SET_score_avg >= 4, "excellent", "standard"),
    sen_level = case_when(
      seniority <= 4 ~ "junior",          
      seniority <= 8 ~ "senior",          
      seniority > 8  ~ "very senior")) |>
  select(course_id, SET_level, sen_level) |>
  count(sen_level, name = "n") |>
  mutate(prop = n / sum(n),
         variable = "Seniority Level",
         level = sen_level) |>
  select(variable, level, n, prop) |>
  kable(caption = "Distribution of Academic Degree",
        col.names = c("Variable", 
                      "Level", 
                      "n", 
                      "prop"), bold = TRUE) |>
  kable_styling(
    bootstrap_options = c("striped", "condensed")) 

```

-   Numeric Example 2

```{r}
#| label: wd-4-numeric-ex-2

ep = rnorm(n = 100, mean = 0, sd = sqrt(1))

#okay hear me out! I am technically creating a numeric variable here by making by epsilon with the random normal distribution function.

```

-   Factor Example 1 (renaming levels)

```{r}
#| label: wd-4-factor-ex-1

#lab 4 question 7

childcare_long <- childcare_long |>
  mutate(
    census_region = fct_reorder(census_region, weekly_price, .fun = median, na.rm = TRUE),
    age_group = factor(age_group) |> 
      fct_recode(
        "Infant"      = "infant",
        "Toddler"     = "toddler",
        "Preschooler" = "preschool"
      )
  )

#need to alter this and add a relevel for the infant, toddler, and preschooler name.

```

-   Factor Example 2 (reordering levels)

```{r}
#| label: wd-4-factor-ex-2

childcare_long <- childcare_long |>
  mutate(census_region = fct_reorder(census_region, weekly_price, .fun = median, na.rm = TRUE),
    age_group = str_to_title(age_group))

```

-   Character (example must use functions from **stringr**)

```{r}
#| label: wd-4-string

#lab 4 question 4

ca_childcare <- ca_childcare |> 
  mutate(county_name = str_remove(county_name, " County")) |> #here
  mutate(census_region = fct_collapse(county_name,
    "Superior California" = superior_counties,
    "North Coast" = north_coast_counties,
    "San Francisco Bay Area" = san_fran_counties,
    "Northern San Joaquin Valley" = n_san_joaquin_counties,
    "Central Coast" = central_coast_counties,
    "Southern San Joaquin Valley" = s_san_joaquin_counties,
    "Inland Empire" = inland_counties,
    "Los Angeles" = la_county,
    "Orange" = orange_county,
    "San Diego/Imperial" = san_diego_imperial_counties))

```

-   Date (example must use functions from **lubridate**)

```{r}
#| label: wd-4-date

sql_symphony_attendees <- facebook_event_checkin |>
 filter(str_detect(event_name, regex("SQL Symphony Concert", ignore_case = TRUE)), year(date) == 2017, month(date) == 12)

```

**WD-5: I can use mutating joins to combine multiple dataframes.**

-   `left_join()` Example 1

```{r}
#| label: wd-5-left-ex-1

ca_childcare <- childcare_costs |> 
                left_join(x = childcare_costs,
                          y = counties,
                          by = join_by(county_fips_code == county_fips_code)) |>
                filter(state_name == "California")

```

-   `right_join()` Example 1

```{r}
#| label: wd-5-right

witness1 <- person |>
  filter(str_detect(address_street_name, regex("Northwestern Dr", ignore_case = TRUE))) |> 
  filter(address_number == max(address_number))

witness2 <- person |>
  filter(str_detect(address_street_name, regex("Franklin Ave", ignore_case = TRUE))) |> 
  filter(str_detect(name, "Annabel"))

witnesses <- bind_rows(witness1, witness)

#so this worked but we can add a right join here for fun by just

witness_join <- right_join(person, witnesses, by ="person_id")

#now it is in one dataframe

```

-   `left_join()` **or** `right_join()` Example 2

```{r}
#| label: wd-5-left-right-ex-2

ca_childcare <- ca_childcare |>
              left_join ( x = ca_childcare,
                          y = tax_rev_clean,
                          by = join_by(county_name == entity_name)) |>
              group_by(county_name)

```

-   `inner_join()` Example 1

```{r}
#| label: wd-5-inner-ex-1

witnesses <- witnesses |>
  inner_join(interview, by = c("id" = "person_id"))
```

-   `inner_join()` Example 2

```{r}
#| label: wd-5-inner-ex-2

suspects_combined <- get_fit_check_suspects |>
  inner_join(get_fit_member_suspects, by = c("membership_id" = "id"))

```

**WD-6: I can use filtering joins to filter rows from a dataframe.**

-   `semi_join()`

```{r}
#| label: wd-6-semi

#suspect list not joining the interview data with suspect but filtering the interview data set 

#semi joins are only looking for matches

```

-   `anti_join()`

```{r}
#| label: wd-6-anti

```

**WD-7: I can pivot dataframes from long to wide and visa versa**

-   `pivot_longer()`

```{r}
#| label: wd-7-long

childcare_long <- ca_childcare |>
  pivot_longer(cols = starts_with("mc_"),  
    names_to = "age_group",
    names_prefix = "mc_",
    values_to = "weekly_price")

```

-   `pivot_wider()`

```{r}
#| label: wd-7-wide

median_household <- ca_childcare |>
  filter(study_year %in% c(2008, 2018)) |>
  group_by(census_region, study_year) |>
  summarise(median_income = median(mhi_2018, na.rm = TRUE), .groups = "drop") |>
  pivot_wider(names_from = study_year, values_from = median_income) |>
  arrange(desc("2018"))


```

## Reproducibility

**R-1: I can create professional looking, reproducible analyses using RStudio projects, Quarto documents, and the here package.**

The following assignments satisfy the above criteria:

-   Example 1
-   Example 2
-   Example 3
-   Example 4
-   Example 5

**R-2: I can write well documented and tidy code.**

-   Example of **ggplot2** plotting

```{r}
#| label: r-2-1

ggplot(data = surveys, 
       mapping = aes(x = weight, y = hindfoot_length)) +
  geom_point(color = "salmon", alpha = 0.5) +
  facet_wrap(~species) +
  labs( x= "Weight (g)",y = NULL, title = "Relationship between weight and hindfoot length across rodent species", subtitle = "Hindfoot Length (mm)") +
  theme(plot.subtitle = element_text(hjust = 0.5))

```

-   Example of **dplyr** pipeline

```{r}
#| label: r-2-2

median_household <- ca_childcare |>
  filter(study_year %in% c(2008, 2018)) |>
  group_by(census_region, study_year) |>
  summarise(median_income = median(mhi_2018, na.rm = TRUE), .groups = "drop") |>
  pivot_wider(names_from = study_year, values_from = median_income) |>
  arrange(desc("2018"))

```

-   Example of function formatting

```{r}
#| label: r-2-3

#Lab 10 Q #8

mycifun <- function(beta0 = 2, beta1 = 1, n = 100) {
  

# generate x vector

x = runif(n, min = 1, max = 100)


# generate noise `ep` vector

ep = rnorm(n = n, mean = 0, sd = sqrt(1))


# generate outcome from population model
y = beta0 + x*beta1 + ep


# create an "observed data" dataframe with only the x and y vectors

observed_data <- tibble(x,y)


observed_data_lm <- lm(y ~ x,
                       data = observed_data) 

extract_observed <- tidy(observed_data_lm,
                         conf.int = TRUE,
                         conf.level = 0.95) |>
                    filter(term == "x") |>
                    select(estimate, conf.low, conf.high) 

extract_observed |>
 mutate(cover = if_else(conf.high > beta1, TRUE, FALSE)) }



```

**R-3: I can write robust programs that are resistant to changes in inputs.**

-   Example (any context)

```{r}
#| label: r-3-example

randomBabies <- function(n_babies){
  
      babies <- rbinom(1, n_babies, 1/n_babies)

}

#important to start with 1 simulation so that we can switch it up
```

-   Example (function stops)

```{r}
#| label: r-3-function-stops

rescale_01 <- function(x) {
    if(!is.numeric(x)) {stop("input vector is not numeric")}

    if(length(x) <=1) {stop("the length of the input vector is not greater than one")}

    # (x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))
  
      r <- range(x, na.rm = TRUE)
        (x - r[1]) / (r[2] - r[1])

}


```

## Data Visualization & Summarization

**DVS-1: I can create visualizations for a *variety* of variable types (e.g., numeric, character, factor, date)**

-   At least two numeric variables

```{r}
#| label: dvs-1-num

ggplot(ca_childcare, aes(x = mhi_2018, y = mc_infant)) +
  geom_point(alpha = 0.6, color = "pink") +
  geom_smooth(method = "lm", se = TRUE, color = "darkgreen") +
  labs(
    title = "Relationship Between Median Household Income and Infant Center-Based Childcare Prices",
    x = "Median Household Income in 2018 ($)",
    y = "Weekly Median Infant Care Price ($)")


```

-   At least one numeric variable and one categorical variable

```{r}
#| label: dvs-2-num-cat

ggplot(data = surveys, 
       mapping = aes(y = species, x = weight)) +
  geom_jitter(color = "grey", alpha = 0.5) +
  geom_boxplot(aes(fill = sex), outliers = FALSE) +
  scale_fill_brewer(palette = 5)
  labs( y= "Species of Rodent",x = "Weight (g)", title = "Relationship between species of rodents and their weight")

```

-   At least two categorical variables

```{r}
#| label: dvs-2-cat

sen_vs_jun |> ggplot(aes(x = sen_level, fill = SET_level)) +
  geom_bar() +
  scale_fill_manual( values = c("standard" = "#C4a484", "excellent" =    "#CBC3E3"))+
  labs(title = "Evaluations of Teacher's Use of Activities", x = "Years of Experience", y = NULL)

```

-   Dates (time series plot)

```{r}
#| label: dvs-2-date

ggplot(summary_fishNA, aes(x = year, y = NAs_inweight, color = section, shape = section)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_smooth(method = "loess", se = FALSE, linewidth = 1) +
  facet_wrap(~ trip) +
  scale_color_manual(values = c("blue", "orange")) +
   labs(title = "Amount of NAs over time by section leader",
    x = "Years",
    y = NULL,
    color = "Section Leader") +
  theme_minimal(base_size = 8) +
  theme(legend.position = "right",aspect.ratio = 1,strip.text = element_text(face = "bold")) +
  theme_gray()
```

**DVS-2: I use plot modifications to make my visualization clear to the reader.**

-   I can modify my plot theme to be more readable

```{r}
#| label: dvs-2-ex-1

```

-   I can modify my colors to be accessible to anyone's eyes

```{r}
#| label: dvs-2-ex-2

```

-   I can modify my plot titles to clearly communicate the data context

```{r}
#| label: dvs-2-ex-3

```

-   I can modify the text in my plot to be more readable

```{r}
#| label: dvs-2-ex-4

#before code chunk, I did not change the font size and I did not separate
ggplot(childcare_long, aes(x = study_year, y = weekly_price, color = census_region)) +
  geom_point(alpha = 0.4, size = 1) +
  geom_smooth(method = "loess", se = FALSE, linewidth = 1) +
  facet_wrap(~ age_group) +
  labs(title = "Weekly Median Price for Center-Based Childcare ($)",
    x = "Study Year",
    y = NULL,
    color = "California Region") +
  theme_minimal(base_size = 9) +
  theme(legend.position = "right",aspect.ratio = 1,strip.text = element_text(face = "bold")) 

#here is what my code should have included along with changing the font size to 9 instead of 12

+ scale_x_continuous(breaks = seq(2008, 2018, by = 2), expand = expansion(mult = c(0.05,0.05))) +
  scale_y_continuous(labels = dollar)
```

-   Before and after
-   
-   ![](images/clipboard-1844204121.png)![](images/clipboard-739741966.png)
-   I can reorder my legend to align with the colors in my plot

```{r}
#| label: dvs-2-ex-5

```

**DVS-3: I show creativity in my visualizations**

-   I can use non-standard colors (Example 1)

```{r}
#| label: dvs-3-1-ex-1

```

-   I can use non-standard colors (Example 2)

```{r}
#| label: dvs-3-1-ex-2

ggplot(mean_fish, aes(x = year, 
                       y = mean_fk, 
                       color = species, 
                       shape = species)) +
  geom_point(alpha = 0.4, size = 2) +
  geom_smooth(method = "loess", se = FALSE, linewidth = 1) +
  scale_color_manual(values = c("WCT" = "coral", 
                                "RBT" = "salmon", 
                                "Bull" = "darkseagreen", 
                                "Brown" = "saddlebrown"))
   labs(title = "Change in Montana Fish Health Over Time (Using Fulton's K)",
    x = "Years",
    y = NULL,
    color = "species") +
  theme_minimal(base_size = 8) +
  theme(legend.position = "right",aspect.ratio = 1,strip.text = element_text(face = "bold")) +
  theme_gray()

```

-   I can use annotations (e.g., `geom_text()`)

```{r}
#| label: dvs-3-2

```

-   I can choose creative geometries (e.g., `geom_segment()`, `geom_ribbon)()`)

```{r}
#| label: dvs-3-3

ci_dat <- ci_dat |>
  mutate(MultiverseNum = 1:100, 
         inside_bounds = conf.low <=1 & conf.high > 1) |> 
  ggplot(aes(x = conf.low, 
             xend = conf.high, 
             y = MultiverseNum, 
             yend = MultiverseNum, 
             color = inside_bounds )) +
  geom_segment() +
   scale_color_manual(values = c(`TRUE` = "darkgreen", `FALSE` = "magenta"))+
 geom_point(aes(x = estimate, y = MultiverseNum), color = "black", size = 1) +
  labs(x = "Slope Coefficient Estimate", y = NULL,
title = "95% Confidence Intervals of Estimated Regression Slope for 100 Multiverses", 
subtitle =
"CI's that do not include the population slope (1) are magenta")

```

**DVS-4: I can calculate numerical summaries of variables.**

-   Example using `summarize()`

```{r}
#| label: dvs-4-summarize

ci_dat |>
summarize(porp = sum(cover)/length(cover))
```

-   Example using `across()`

```{r}
#| label: dvs-4-across

fish |>
  select(everything()) |>  
  summarize(across(everything(), ~ sum(is.na(.))))

```

**DVS-5: I can find summaries of variables across multiple groups.**

-   Example 1

```{r}
#| label: dvs-5-1

frequent_sql_attendees <- sql_symphony_attendees |>
  group_by(person_id) |>
  summarise(times_attended = n()) |>
  filter(times_attended >= 3)

```

-   Example 2

```{r}
#| label: dvs-5-2

teacher_evals_clean |>
  filter(academic_degree %in% c("dr", "prof")) |>
  group_by(teacher_id, seniority, sex) |>
  summarise(avg_response_rate = mean(resp_share, na.rm = TRUE), 
            .groups = "drop") |>
  slice_min(avg_response_rate, n = 1) |>
  bind_rows(teacher_evals_clean |>
      filter(academic_degree %in% c("dr", "prof")) |>
      group_by(teacher_id, seniority, sex) |>
      summarise(avg_response_rate = mean(resp_share, na.rm = TRUE), 
                .groups = "drop") |>
      slice_max(avg_response_rate, n = 1))

```

**DVS-6: I can create tables which make my summaries clear to the reader.**

-   I can modify my column names to clearly communicate the data context

```{r}
#| label: dvs-6-ex-1

fish_NA_sum <-map_int(fish, ~sum(is.na(.x))) |>
              enframe(name = "Variable", value = "NA Counts")

```

-   I can modify the text in my table to be more readable (e.g., bold face for column headers)

```{r}
#| label: dvs-6-ex-2

surveys |>
        map_chr(class) |>
        enframe(name = "Variable", value = "Data Type") |>
        arrange("Variable") |>
        kable(caption = "Summary of Data Types in Surveys Dataset") |>
        kable_styling() |>
        row_spec(0, bold = TRUE)

#the kable() function already bolds automatically but kable_styling helps narrow down which row we want bolded

```

-   I can arrange my table to have an intuitive ordering

```{r}
#| label: dvs-6-ex-3

surveys |>
        map_chr(class) |>
        enframe(name = "Variable", value = "Data Type") |>
        arrange("Variable") |> #here
        kable(caption = "Summary of Data Types in Surveys Dataset")

```

**DVS-7: I show creativity in my tables.**

-   I can use non-default colors

```{r}
#| label: dvs-7-ex-1

```

-   I can modify the layout of my table to be more readable (e.g., `pivot_longer()` or `pivot_wider()`)

```{r}
#| label: dvs-7-ex-2

pivot_table <- function(data, rows, cols) {
  data |> 
    count(pick(c({{ rows }}, {{ cols }}))) |> 
    pivot_wider(
      names_from = {{ cols }}, 
      values_from = n, #n is coming from the result of count()
      names_sort = TRUE,
      values_fill = 0
    )
}

```

## Program Efficiency

**PE-1: I can write concise code which does not repeat itself.**

-   using a single function call with multiple inputs (rather than multiple function calls)

```{r}
#| label: pe-1-one-call

# witness1 <- person |>
#   filter(str_detect(address_street_name, regex("Northwestern Dr", ignore_case = TRUE))) |> 
#   filter(address_number == max(address_number))

# witness2 <- person |>
#   filter(str_detect(address_street_name, regex("Franklin Ave", ignore_case = TRUE))) |> 
#   filter(str_detect(name, "Annabel"))

# witnesses <- bind_rows(witness1, witness2)

#the code above could be fixed by maximizing the use of the filter function. We could instead combine the filters per witness instead of a new filter line for each row. 

witness1 <- person |>
  filter(
    str_detect(address_street_name, "Northwestern Dr"),
    address_number == max(address_number)
  )

witness2 <- person |>
  filter(
    str_detect(address_street_name, "Franklin Ave"),
    str_detect(name, "Annabel")
  )

witnesses <- bind_rows(witness1, witness2)


```

-   using `across()`

```{r}
#| label: pe-1-across

tax_rev_clean <- tax_rev |>
                group_by(entity_name) |>
                summarise(across(everything(), first), .groups = "drop")

```

-   using functions from the `map()` family

```{r}
#| label: pe-1-map-1

#Lab 9 Q4
#Success comment: 
# Do you need to select for this to work? Can map_at() work on the entire dataset?

teacher_evals_clean <- evals |>
 # rename(sex = gender) |> 
  filter(no_participants > 10) |>
  mutate(teacher_id = as.character(teacher_id),
         question_no = as.character(question_no)) |>
  select("course_id", "teacher_id", "question_no", "no_participants", "resp_share", "SET_score_avg", "percent_failed_cur", "academic_degree", "seniority", "sex") |>
  map_at(c("course_id", #new code starts here, chef
           "weekday", 
           "academic_degree", 
           "time_of_day", 
            "sex"),
          as.factor) |>
  bind_cols()

#not at this point! Just because we are just being asked to convert some columns to factors.

teacher_evals_clean <- evals |>
 # rename(sex = gender) |> 
  filter(no_participants > 10) |>
  mutate(teacher_id = as.character(teacher_id),
         question_no = as.character(question_no)) |>
  map_at(c("course_id", #new code starts here, chef
           "weekday", 
           "academic_degree", 
           "time_of_day", 
            "sex"),
          as.factor) |>
  bind_cols()

#we can later select or remove columns we do or don't need for later analyses
```

**PE-2: I can write functions to reduce repetition in my code.**

-   Example 1: Function that operates on vectors

```{r}
#| label: pe-2-1

imposs_NA <- function(x, min, max) {
  case_when(
    x < min ~ NA,
    x > max ~ NA,
    .default = x
  )
}

imposs_NA(1:10, min = 3, max = 7)

```

-   Example 2: Function that operates on data frames

```{r}
#| label: pe-2-2

rescale_column <- function(df, cols){
  df |> 
    mutate(
      across(.cols = {{ cols }}, 
             .fns = ~ rescale_01(.x)
             )
      )
}

```

-   Example 3: Function that operates on vectors *or* data frames

```{r}
#| label: pe-2-3

plot_function <- function(df, x, y, color) {
  
  label <- rlang::englue("Flipper length vs Body mass across penguin species")
  
  df |>
    ggplot(aes(x = {{ x }}, y = {{ y }}, color = {{ color }})) +
    geom_point() +
    labs(title = label, 
         x = " Flipper length (mm)", 
         y = " Body mass (g) ")
  
}
```

**PE-3:I can use iteration to reduce repetition in my code.**

-   using `across()`

```{r}
#| label: pe-3-across


rescale_column <- function(df, cols){
  df |> 
    mutate(
      across(.cols = {{ cols }}, 
             .fns = ~ rescale_01(.x)
             )
      )
}
```

-   using a `map()` function with **one** input (e.g., `map()`, `map_chr()`, `map_dbl()`, etc.)

```{r}
#| label: pe-3-map-1

surveys |>
        map_chr(class) |>
        enframe(name = "Variable", value = "Data Type") |>
        arrange("Variable") |>
        kable(caption = "Summary of Data Types in Surveys Dataset",
              bold = TRUE)

```

-   using a `map()` function with **one** input

```{r}
#| label: pe-3-map-2

ci_dat <- map(.x = 1:1000,
              .f = ~mycifun(beta0 = 3, beta1 = 0.5, n = 100)
              )

```

**PE-4: I can use modern tools when carrying out my analysis.**

-   I can use functions which are not superseded or deprecated

```{r}
#| label: pe-4-1

```

-   I can connect a data wrangling pipeline into a `ggplot()`

```{r}
#| label: pe-4-2

ci_dat <- ci_dat |>
  map(.x = 1:100,
      .f = ~mycifun(beta0 = 0, beta1 = 1, n = 100)) |>
  bind_rows() |>
  mutate(MultiverseNum = 1:100, 
         inside_bounds = conf.low <=1 & conf.high > 1) |> 
  ggplot(aes(x = conf.low, 
             xend = conf.high, 
             y = MultiverseNum, 
             yend = MultiverseNum, 
             color = inside_bounds )) +
  geom_segment() +
   scale_color_manual(values = c(`TRUE` = "darkgreen", `FALSE` = "magenta"))+
 geom_point(aes(x = estimate, y = MultiverseNum), color = "black", size = 1) +
  labs(x = "Slope Coefficient Estimate", y = NULL,
title = "95% Confidence Intervals of Estimated Regression Slope for 100 Multiverses", 
subtitle =
"CI's that do not include the population slope (1) are magenta")

```

## Data Simulation & Statistical Models

**DSSM-1: I can simulate data from a *variety* of probability models.**

-   Example 1

```{r}
#| label: dsm-1-1

randomBabies <- function(n_babies){
  
      babies <- rbinom(1, n_babies, 1/n_babies)

}
  
set.seed(918)

results <- map_int(.x = 1:10000,
                   .f = ~ randomBabies(n_babies = 4)
                  )

table(results)

```

-   Example 2

```{r}
#| label: dsm-1-2

mycifun <- function(beta0 = 2, beta1 = 1, n = 100) {
  

# generate x vector

x = runif(n, min = 1, max = 100)


# generate noise `ep` vector

ep = rnorm(n = n, mean = 0, sd = sqrt(1))


# generate outcome from population model
y = beta0 + x*beta1 + ep


# create an "observed data" dataframe with only the x and y vectors

observed_data <- tibble(x,y)


observed_data_lm <- lm(y ~ x,
                       data = observed_data) 

extract_observed <- tidy(observed_data_lm,
                         conf.int = TRUE,
                         conf.level = 0.95) |>
                    filter(term == "x") |>
                    select(estimate, conf.low, conf.high) 

extract_observed |>
 mutate(cover = if_else(conf.high > beta1, TRUE, FALSE)) }

```

**DSSM-2: I can conduct common statistical analyses in R.**

-   Example 1

```{r}
#| label: dsm-2-1

species_mod <- aov(weight ~ species, data = surveys)

#response ~ predictor

summary(species_mod)

#lab 2 question 17

```

-   Example 2

```{r}
#| label: dsm-2-2

#chi_data <- table(sen_vs_jun$sen_level, sen_vs_jun$SET_level)

# view(chi_data)

# chisq.test(chi_data)

#Growing Comment: This works, but did you need to make a contingency table? Could you have input these vectors directly into the chisq.test() function?

chisq.test(sen_vs_jun$sen_level, sen_vs_jun$SET_level)

#Pearson's Chi-squared test

# data:  sen_vs_jun$sen_level and sen_vs_jun$SET_level
# X-squared = 10.207, df = 2, p-value = 0.006075

```

-   Example 3

```{r}
#| label: dsm-2-3

t.test(ToothGrowth$len)

#Growing comment: Comments
#This is not a t-test for the difference in means between two groups. This test needs to have two variables (1) the length of the teeth, and (2) the group that the guinea pig was in.

#You also need to make sure you are using unequal variances.

#so I need to alter my code to include a ~ variable or group the guinea pigs were in and tell it that I have unequal variances (var.equal = FALSE). 

t.test(len ~ supp, data = ToothGrowth, var.equal = FALSE)

# 	Welch Two Sample t-test
# 
# data:  len by supp
# t = 1.9153, df = 55.309, p-value = 0.06063
# alternative hypothesis: true difference in means between group OJ and group VC is not equal to 0
# 95 percent confidence interval:
#  -0.1710156  7.5710156
# sample estimates:
# mean in group OJ mean in group VC 
#         20.66333         16.96333 

#Lab 1 Q #10

```

## Revising My Thinking

<!-- How did you revise your thinking throughout the course? How did you revise your thinking on the code examples you have provided in your portfolio? -->

I think I am quick to be satisfied if the code runs and gets me moving forward. It's nice to be given the resources on how to own my code but I still think I am in a position where I am insecure with my abilities to even start a line without referring to the cheat sheets or looking at previous work.

<!-- For the revisions included in your Portfolio, to help me understand the nature of your revisions, please denote somehow the feedback I provided you (e.g., boldface, italics, colored text) before your revisions. -->

## Extending My Thinking

<!-- How did you extended your thinking throughout the course? How did you extend your thinking on the code examples you have provided in your portfolio? -->

I think that a lot of my thought process with coding was that efficiency and customization comes last compared to having just the code run. So a big challenge with this course was making space for finding ways that my code could be more professional looking or colorful and not only being satisfied with the code running and getting an answer. I think a lot of lab 5 made me realize like how I should be doing things instead of trying to get a glimpse of an answer by binding rows or searching through the dataframe itself.

## Peer Support & Collaboration

<!-- Include an image or a description of feedback you gave that you are proud of (either in a peer review or in Discord). -->

I have not been supportive of my peers enough to provide evidence of quality feedback. I look forward to changing this response as the final portfolio approaches.

<!-- Include a description of how you grew as a collaborator through the weekly pair programming activities.   -->

I think this varies because either I am confident in solving the task and can keep up with like a CS major or a more R savvy person or I take too long to think of a line of code to tell the coder and they just take over because class is ending in soon. So I could be faster and give my partner more confidence so that doesn't happen again. At the same time though I've had a partner that was shy and preferred that I took over but then that gives me space to be slow and take my time thinking of code or looking for references.
